{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cv_project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3tUSEMK1QCid"
      ],
      "machine_shape": "hm",
      "mount_file_id": "11QSQX5x8VIYpww6IyIS9no8MbACNXUjk",
      "authorship_tag": "ABX9TyOkCGK/s6ga3TEhZNe7Bwtz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dntjr41/CV_TermP/blob/main/cv_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 드라이브 마운트"
      ],
      "metadata": {
        "id": "Vdkv0kLjHYu3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPUS5g9Z421E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "843ecf8a-48bc-4ad6-ef14-43b91c3e1cde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 경로 지정\n"
      ],
      "metadata": {
        "id": "8V3BKwj8HL3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/My\\ Drive/2022_cv_project\n",
        "!pwd\n",
        "!ls -la"
      ],
      "metadata": {
        "id": "2GPZY2EsHf5X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38f10a58-7e15-4023-f86b-1ea8e56a8f50"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'drive/My Drive/2022_cv_project'\n",
            "/content/drive/My Drive/2022_cv_project\n",
            "/content/drive/MyDrive/2022_cv_project\n",
            "total 4442002\n",
            "drwx------ 3 root root       4096 May 23 08:54 checkpoints\n",
            "drwx------ 3 root root       4096 May 23 10:09 cv_project\n",
            "-rw------- 1 root root      24184 May 24 02:35 cv_project.ipynb\n",
            "drwx------ 2 root root       4096 May 23 14:42 cv_project_na\n",
            "-rw------- 1 root root  454219025 May 23 14:36 cv_project_na.zip\n",
            "-rw------- 1 root root 4094316129 May 23 09:54 cv_project.zip\n",
            "drwx------ 3 root root       4096 May 23 08:42 data\n",
            "drwx------ 2 root root       4096 May 23 13:41 .ipynb_checkpoints\n",
            "drwx------ 2 root root       4096 May 23 10:09 __MACOSX\n",
            "drwx------ 5 root root       4096 May 23 08:42 model\n",
            "drwx------ 5 root root       4096 May 23 08:54 outputs\n",
            "drwx------ 2 root root       4096 May 23 14:49 __pycache__\n",
            "drwx------ 3 root root       4096 May 23 14:42 pytorch_ssim\n",
            "-rw------- 1 root root       1637 May 23 15:09 test.py\n",
            "-rw------- 1 root root       4716 May 23 09:47 train.py\n",
            "-rw------- 1 root root       1294 May 23 16:54 utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 압축 풀기"
      ],
      "metadata": {
        "id": "d-aoW_cLHMxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "import zipfile\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# # 폴더 복사 하기\n",
        "# import shutil\n",
        "# shutil.copytree('./cv_project', './cv_project_na')\n",
        "\n",
        "# # 파일 크기 확인\n",
        "# filepaths = os.listdir('./cv_project/train')\n",
        "# print(len(filepaths))\n",
        "# filepaths = os.listdir('./cv_project/val')\n",
        "# print(len(filepaths))\n",
        "# filepaths = os.listdir('./cv_project/mask')\n",
        "# print(len(filepaths))\n",
        "# filepaths = os.listdir('./cv_project/hint')\n",
        "# print(len(filepaths))\n",
        "\n",
        "# 압축 풀기\n",
        "file_name = 'cv_project_na'\n",
        "!unzip -qq '{file_name}'\n",
        "\n",
        "# 파일 크기 확인\n",
        "filepaths = os.listdir('./cv_project_na/train')\n",
        "print(len(filepaths))\n",
        "filepaths = os.listdir('./cv_project_na/val')\n",
        "print(len(filepaths))\n",
        "filepaths = os.listdir('./cv_project_na/mask')\n",
        "print(len(filepaths))\n",
        "filepaths = os.listdir('./cv_project_na/hint')\n",
        "print(len(filepaths))"
      ],
      "metadata": {
        "id": "ZCZZIgQ7GGAS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b33c953-1590-48e1-a99e-b0adf1e6f8d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n",
            "2000\n",
            "1000\n",
            "1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMMqmdiYMkvi"
      },
      "source": [
        "## 더 빠른 GPU\n",
        "\n",
        "Colab Pro를 사용하여 가장 빠른 GPU에 우선적으로 액세스하세요. Pro+의 경우 더 빠릅니다. 예를 들어 대부분의 표준 Colab 사용자가 속도가 느린 K80 GPU를 수신할 때 Colab Pro 사용자는 T4 또는 P100 GPU를 이용할 수 있습니다. 언제든지 다음 셀을 실행하여 할당된 GPU를 확인할 수 있습니다.\n",
        "\n",
        "아래 코드 셀의 실행 결과가 ‘Not connected to a GPU’인 경우 메뉴의 런타임 &gt; 런타임 유형 변경에서 런타임을 변경하여 GPU 가속기를 사용 설정한 다음 코드 셀을 다시 실행하면 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "23TOba33L4qf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec58a158-d172-4993-e09c-dc47fafeb796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue May 24 02:36:58 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    27W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65MSuHKqNeBZ"
      },
      "source": [
        "## 추가 메모리\n",
        "\n",
        "<p>Colab Pro를 구독하면 고용량 메모리 VM에 액세스할 수 있습니다&#40;사용 가능한 경우&#41;. Pro+에는 더 많은 메모리가 제공됩니다. 고용량 메모리 런타임을 사용하도록 노트북 환경설정을 지정하려면 런타임 &gt; '런타임 유형 변경' 메뉴를 선택한 다음 런타임 구성 드롭다운에서 고용량 RAM을 선택하세요.</p>\n",
        "<p>언제든지 다음 코드 셀을 실행하여 사용 가능한 메모리 용량을 확인할 수 있습니다.</p>\n",
        "아래 코드 셀의 실행 결과가 ‘Not using a high-RAM runtime’인 경우 메뉴의 런타임 &gt; 런타임 유형 변경에서 고용량 RAM 런타임을 사용 설정하고 런타임 구성 드롭다운에서 고용량 RAM을 선택한 다음 코드 셀을 다시 실행하면 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "V1G82GuO-tez",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83dac811-8784-4de1-fbd5-8b36304c1030"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install pytorch-ssim, mssim\n",
        "<p> pip install pytorch_msssim </p>\n",
        "<p> pip install pytorch_ssim </p>"
      ],
      "metadata": {
        "id": "uCGTvywUb96y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pytorch_ssim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jqh8akdWbq5y",
        "outputId": "c020e82c-177b-4dad-f945-e958b1cede07"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch_ssim\n",
            "  Downloading pytorch_ssim-0.1.tar.gz (1.4 kB)\n",
            "Building wheels for collected packages: pytorch-ssim\n",
            "  Building wheel for pytorch-ssim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-ssim: filename=pytorch_ssim-0.1-py3-none-any.whl size=2026 sha256=e4cfc8c26ae9bf88a9f9f468e85cb0fa04250af49905d41962b7c9e5c938dd2e\n",
            "  Stored in directory: /root/.cache/pip/wheels/ee/20/09/ebf5e58bdf2560c760074cd140b7f7b0c882e216feabf1ae30\n",
            "Successfully built pytorch-ssim\n",
            "Installing collected packages: pytorch-ssim\n",
            "Successfully installed pytorch-ssim-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "cA_Dji7vKt5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from data.dataset import ColorHintDataset\n",
        "import torch.utils.data as data\n",
        "import torch\n",
        "import cv2\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from data.transform import tensor2im\n",
        "from model.res_unet.res_unet import ResUnet\n",
        "from model.res_unet.res_unet_plus import ResUnetPlusPlus\n",
        "from model.res_unet.unet import UNet\n",
        "import matplotlib.image as img\n",
        "import copy, time\n",
        "from model.sm.model import ResAttdU_Net\n",
        "from utils import AverageMeter, ssim, save_img\n",
        "\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "  device = \"cuda:0\"\n",
        "  print('device 0 :', torch.cuda.get_device_name(0))\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Change to your data root directory\n",
        "    root_path = \"./cv_project\"\n",
        "\n",
        "    check_path = './checkpoints/'\n",
        "    # Depend on runtime setting\n",
        "    use_cuda = True\n",
        "\n",
        "    # make the directory\n",
        "    os.makedirs('./checkpoints/', exist_ok=True)\n",
        "    os.makedirs('./outputs/', exist_ok=True)\n",
        "    os.makedirs('./outputs/GroundTruth', exist_ok=True)\n",
        "    os.makedirs('./outputs/Hint', exist_ok=True)\n",
        "    os.makedirs('./outputs/Output', exist_ok=True)\n",
        "    os.makedirs('./checkpoints', exist_ok=True)\n",
        "\n",
        "    # Load the data\n",
        "    train_dataset = ColorHintDataset(root_path, 256, \"train\")\n",
        "    val_dataset = ColorHintDataset(root_path, 256, \"val\")\n",
        "\n",
        "    dataloaders = {}\n",
        "    dataloaders['train'] = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "    dataloaders['valid'] = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "\n",
        "    print('train dataset: ', len(train_dataset))\n",
        "    print('validation dataset: ', len(val_dataset))\n",
        "\n",
        "\n",
        "    # Select the model\n",
        "    models = {'ResUnet': ResUnet(3), 'ResUnetPlusPlus': ResUnetPlusPlus(3), 'UNet': UNet(), 'ResAttdUnet' : ResAttdU_Net()}\n",
        "    model = models['ResUnetPlusPlus'].to(device)\n",
        "    model.load_state_dict(torch.load('./checkpoints/model-epoch-1-losses-0.09352.pth'))\n",
        "\n",
        "    criterion = torch.nn.L1Loss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "    lmbda = lambda epoch : 0.95\n",
        "    exp_lr_scehduler = optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)\n",
        "    epochs = 10\n",
        "\n",
        "    # initialize the\n",
        "    since = time.time()\n",
        "    train_loss, train_acc, valid_loss, valid_acc = [], [], [], []\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = 999\n",
        "    for epoch in range(epochs):\n",
        "        print('Epoch {}/{}'.format(epoch + 1, epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'valid']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()  # Set model to evaluate mode\n",
        "\n",
        "            losses = AverageMeter()\n",
        "\n",
        "            # Iterate over data.\n",
        "            for i, data in enumerate(tqdm.tqdm(dataloaders[phase])):\n",
        "                if use_cuda:\n",
        "                    l = data[\"l\"].to(device)\n",
        "                    ab = data[\"ab\"].to(device)\n",
        "                    hint = data[\"hint\"].to(device)\n",
        "                else:\n",
        "                    l = data[\"l\"]\n",
        "                    ab = data[\"ab\"]\n",
        "                    hint = data[\"hint\"]\n",
        "\n",
        "                gt_image = torch.cat((l, ab), dim=1)\n",
        "                hint_image = torch.cat((l, hint), dim=1)\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(hint_image)\n",
        "                    loss = criterion(outputs, gt_image)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        # zero the parameter gradients\n",
        "                        optimizer.zero_grad()\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                losses.update(loss.item(), hint_image.size(0))\n",
        "\n",
        "                if phase == 'train':\n",
        "                  if i % 1000 == 0:\n",
        "                    print('\\t Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(loss=losses))\n",
        "                \n",
        "                else:\n",
        "                  out_hint_np = tensor2im(outputs)\n",
        "                  out_hint_bgr = cv2.cvtColor(out_hint_np, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "                  hint_np = tensor2im(hint_image)\n",
        "                  hint_bgr = cv2.cvtColor(hint_np, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "                  gt_np = tensor2im(gt_image)\n",
        "                  gt_bgr = cv2.cvtColor(gt_np, cv2.COLOR_LAB2BGR)\n",
        "                  save_img(gt_bgr, hint_bgr, out_hint_bgr, i)\n",
        "\n",
        "            if phase == 'train':\n",
        "                exp_lr_scheduler.step()\n",
        "                train_loss.append(losses)\n",
        "\n",
        "            else:\n",
        "                valid_loss.append(losses)\n",
        "\n",
        "            print(' {} Loss: {:.3f} '.format(phase, losses.avg))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'valid' and losses.avg < best_loss:\n",
        "                best_idx = epoch\n",
        "                best_loss = losses.avg\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "                # Save model & checkpoint\n",
        "                torch.save(model.state_dict(), './checkpoints/model-epoch-{}-losses-{:.5f}.pth'.format(epoch + 1, best_loss))\n",
        "\n",
        "                print('==> best model saved - %d / %.3f' % (best_idx, best_loss))\n",
        "\n",
        "    # Training Result\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best valid Acc: %d - %.4f' % (best_idx, best_loss))\n",
        "\n",
        "\n",
        "    # Plot the training procedure\n",
        "    epoch_axis = np.arange(0, epochs)\n",
        "    plt.figure()\n",
        "    plt.title('LOSS')\n",
        "    plt.plot(epoch_axis, train_loss, epoch_axis, valid_loss, 'r-')\n",
        "    plt.legend(['Train', 'Validation'], loc='best')\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "_SrUL7biKx7i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e82caeaa-6963-4ca6-fd4a-10dd776b94a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device 0 : Tesla P100-PCIE-16GB\n",
            "train dataset:  40000\n",
            "validation dataset:  2000\n",
            "Epoch 1/10\n",
            "----------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/10000 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([4, 3, 256, 256])) that is different to the input size (torch.Size([4, 1, 256, 256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n",
            "  0%|          | 1/10000 [00:01<4:53:16,  1.76s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t Loss 0.0970 (0.0970)\t\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 1001/10000 [26:41<3:57:41,  1.58s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t Loss 0.0948 (0.0937)\t\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 2001/10000 [52:55<3:33:11,  1.60s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t Loss 0.1084 (0.0940)\t\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 3001/10000 [1:18:45<2:54:45,  1.50s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t Loss 0.0899 (0.0939)\t\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 4001/10000 [1:44:44<2:34:52,  1.55s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t Loss 0.0935 (0.0939)\t\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 5001/10000 [2:10:34<2:07:11,  1.53s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t Loss 0.0949 (0.0938)\t\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 6001/10000 [2:36:23<1:47:20,  1.61s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t Loss 0.1007 (0.0937)\t\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 7001/10000 [3:02:13<1:17:39,  1.55s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t Loss 0.0881 (0.0936)\t\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 8001/10000 [3:28:41<51:44,  1.55s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t Loss 0.0842 (0.0935)\t\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 9001/10000 [3:54:43<25:56,  1.56s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t Loss 0.0876 (0.0936)\t\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [4:20:47<00:00,  1.56s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " train Loss: 0.094 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 46%|████▌     | 230/500 [08:16<09:34,  2.13s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "3tUSEMK1QCid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from data.dataset import ColorHintDataset\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import cv2\n",
        "import tqdm\n",
        "import os\n",
        "from data.transform import tensor2im\n",
        "from model.res_unet.res_unet import ResUnet\n",
        "from model.res_unet.res_unet_plus import ResUnetPlusPlus\n",
        "from model.res_unet.unet import UNet\n",
        "\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "  device = \"cuda:0\"\n",
        "  print('device 0 :', torch.cuda.get_device_name(0))\n",
        "  \n",
        "def main():\n",
        "    # Change to your data root directory\n",
        "    root_path = \"./cv_project\"\n",
        "    check_point = './checkpoints/best_model.pt'\n",
        "\n",
        "    # Depend on runtime setting\n",
        "    use_cuda = True\n",
        "\n",
        "    test_dataset = ColorHintDataset(root_path, 256, \"test\")\n",
        "\n",
        "    dataloaders = {}\n",
        "    dataloaders['test'] = torch.utils.data.DataLoader(test_dataset, shuffle=False)\n",
        "    print('test dataset: ', len(test_dataset))\n",
        "\n",
        "\n",
        "    # state_dict = torch.load(check_point)\n",
        "    model = ResUnet(3).to(device)\n",
        "\n",
        "    model.load_state_dict(torch.load(check_point))\n",
        "\n",
        "    os.makedirs('outputs/test', exist_ok=True)\n",
        "\n",
        "    model.eval()\n",
        "    for i, data in enumerate(tqdm.tqdm(dataloaders['test'])):\n",
        "        if use_cuda:\n",
        "            l = data[\"l\"].to(device)\n",
        "            hint = data[\"hint\"].to(device)\n",
        "            mask = data[\"mask\"].to(device)\n",
        "        else:\n",
        "            l = data[\"l\"]\n",
        "            hint = data[\"hint\"]\n",
        "            mask = data[\"mask\"]\n",
        "\n",
        "        filename = data[\"file_name\"]\n",
        "\n",
        "        hint_image = torch.cat((l, hint), dim=1)\n",
        "\n",
        "        output_hint = model(hint_image)\n",
        "        out_hint_np = tensor2im(output_hint)\n",
        "        output_bgr = cv2.cvtColor(out_hint_np, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "        fname = str(filename).replace(\"['\", '')\n",
        "        fname = fname.replace(\"']\", '')\n",
        "\n",
        "        cv2.imwrite('./outputs/test/' + str(fname), output_bgr)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "6O4g_L2UQEEL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2eb71509-2e7b-461c-b558-13dc833dced8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device 0 : Tesla P100-PCIE-16GB\n",
            "test dataset:  1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:38<00:00, 26.25it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict"
      ],
      "metadata": {
        "id": "Qr3vTEC-SHGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from data.dataset import ColorHintDataset\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import cv2\n",
        "import tqdm\n",
        "import os\n",
        "from data.transform import tensor2im\n",
        "from model.res_unet.res_unet import ResUnet\n",
        "from model.res_unet.res_unet_plus import ResUnetPlusPlus\n",
        "from model.res_unet.unet import UNet\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pylab\n",
        "from test import test\n",
        "\n",
        "def main():\n",
        "    ## calculate and save psnr, ssim ##\n",
        "    with torch.no_grad():\n",
        "      test()\n",
        "    # change to your Output data directory\n",
        "    output_path = \"./outputs/Output\"\n",
        "    file_list = os.listdir(output_path)\n",
        "\n",
        "    ssim = np.zeros(len(file_list))\n",
        "    psnr = np.zeros(len(file_list))\n",
        "\n",
        "    for i, img_name in enumerate(file_list):\n",
        "        # print(img_name)\n",
        "        name = img_name.replace('.png', '')   # remove '.png'\n",
        "        temp = name.split('_')\n",
        "        ssim[i] += float(temp[1].replace('ssim:', ''))\n",
        "        psnr[i] += float(temp[2].replace('psnr:', ''))\n",
        "\n",
        "    ssim_avg = sum(ssim)/len(ssim)\n",
        "    psnr_avg = sum(psnr)/len(psnr)\n",
        "\n",
        "    print('Average of ssim: {}'.format(ssim_avg))\n",
        "    print('Average of psnr: {}'.format(psnr_avg))\n",
        "\n",
        "    np.save(os.path.join('./', 'ssim.npy'), ssim)\n",
        "    np.save(os.path.join('./', 'psnr.npy'), psnr)\n",
        "\n",
        "    # plot and save ssim curve\n",
        "    plt.figure()\n",
        "    plt.title('ssim')\n",
        "    pylab.xlim(0, len(file_list) + 1)\n",
        "    pylab.ylim(0, 1.1)\n",
        "    plt.plot(range(1, len(file_list) + 1), ssim, label='ssim')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join('./', 'ssim.pdf'))\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    # plot and save psnr curve\n",
        "    plt.figure()\n",
        "    plt.title('pnsr')\n",
        "    pylab.xlim(0, len(file_list) + 1)\n",
        "    pylab.ylim(0, 100)\n",
        "    plt.plot(range(1, len(file_list) + 1), psnr, label='psnr')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join('./', 'psnr.pdf'))\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "HXkPU0JkSJRj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}